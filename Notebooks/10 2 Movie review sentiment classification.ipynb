{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Uncomment these lines to deactivate a GPU\n",
    "#\n",
    "# As the problem size here is very small, training on a CPU might be faster\n",
    "#\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "    \n",
    "import keras\n",
    "import KerasTools as KT\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 1000 # Top most frequent words to consider\n",
    "maxlen = 500        # Cut texts after this number of words\n",
    "\n",
    "print('Load data...')\n",
    "(train_data, train_labels), (test_data, test_labels) = KT.datasets.imdb.load_data(num_words=max_features)\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(train_data, maxlen=maxlen)\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(test_data, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Source: https://mlwhiz.com/blog/2018/12/17/text_classification/\n",
    "#\n",
    "\n",
    "def textcnn(inp):\n",
    "    filter_sizes = [1,2,3,5]\n",
    "    num_filters = 32\n",
    "\n",
    "    maxpool_pool = []\n",
    "    for i in range(len(filter_sizes)):\n",
    "        conv = keras.layers.Conv1D(\n",
    "            num_filters, kernel_size=filter_sizes[i],\n",
    "            kernel_initializer='he_normal', activation='relu', padding='causal'\n",
    "        )(inp)\n",
    "        maxpool_pool.append(\n",
    "            keras.layers.MaxPool1D(pool_size=(maxlen - filter_sizes[i] + 1))(conv)\n",
    "        )\n",
    "    z = keras.layers.Concatenate(axis=1)(maxpool_pool)\n",
    "    z = keras.layers.Flatten()(z)\n",
    "    return z\n",
    "    \n",
    "class AttentionWithContext(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "    \"Hierarchical Attention Networks for Document Classification\"\n",
    "    by using a context vector to assist the attention\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    How to use:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    "    Note: The layer has been tested with Keras 2.0.6\n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(AttentionWithContext())\n",
    "        # next add a Dense layer (for classification/regression) or whatever...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = keras.initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = keras.regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = keras.regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = keras.regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = keras.constraints.get(W_constraint)\n",
    "        self.u_constraint = keras.constraints.get(u_constraint)\n",
    "        self.b_constraint = keras.constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "\n",
    "        self.u = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def _dot_product(self, x, kernel):\n",
    "        return keras.backend.squeeze(keras.backend.dot(x, keras.backend.expand_dims(kernel)), axis=-1)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = self._dot_product(x, self.W)\n",
    "        if self.bias: uit += self.b\n",
    "        uit = keras.backend.tanh(uit)\n",
    "        ait = self._dot_product(uit, self.u)\n",
    "\n",
    "        a = keras.backend.exp(ait)\n",
    "        if mask is not None: a *= mask\n",
    "        a /= keras.backend.sum(a, axis=1, keepdims=True) + keras.backend.epsilon()\n",
    "        a = keras.backend.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        \n",
    "        return keras.backend.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(intermediate):\n",
    "    inp = keras.layers.Input(shape=(maxlen, ))\n",
    "    embd = keras.layers.Embedding(max_features, 3, mask_zero=False)(inp)\n",
    "    intm = intermediate(embd)\n",
    "    out = keras.layers.Dense(1, activation='sigmoid')(intm)\n",
    "    model = keras.models.Model(inputs=inp, outputs=out)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.RMSprop(), metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 55.7 s on Intel i7-7700HQ CPU @ 2.80GHz \n",
    "# 22.7 s on Intel i7-7700HQ CPU @ 2.80GHz / Nvidia GeForce 940MX\n",
    "model = build_network(lambda x: keras.layers.Dense(32, activation='relu')(\n",
    "                                keras.layers.Flatten()(x)))\n",
    "history_embd = model.fit(x_train, train_labels, batch_size=256, epochs=50, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#  on Intel i7-7700HQ CPU @ 2.80GHz\n",
    "#  on Intel i7-7700HQ CPU @ 2.80GHz / Nvidia GeForce 940MX\n",
    "model = build_network(keras.layers.SimpleRNN(32))\n",
    "history_simplernn = model.fit(x_train, train_labels, batch_size=256, epochs=50, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#  on Intel i7-7700HQ CPU @ 2.80GHz\n",
    "#  on Intel i7-7700HQ CPU @ 2.80GHz / Nvidia GeForce 940MX\n",
    "model = build_network(keras.layers.GRU(32))\n",
    "history_gru = model.fit(x_train, train_labels, batch_size=256, epochs=50, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#  on Intel i7-7700HQ CPU @ 2.80GHz\n",
    "#  on Intel i7-7700HQ CPU @ 2.80GHz / Nvidia GeForce 940MX\n",
    "model = build_network(keras.layers.Bidirectional(keras.layers.GRU(32)))\n",
    "history_bigru = model.fit(x_train, train_labels, batch_size=256, epochs=50, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#  on Intel i7-7700HQ CPU @ 2.80GHz\n",
    "#  on Intel i7-7700HQ CPU @ 2.80GHz / Nvidia GeForce 940MX\n",
    "model = build_network(keras.layers.LSTM(32))\n",
    "history_lstm = model.fit(x_train, train_labels, batch_size=256, epochs=50, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 31min 59s on Intel i7-7700HQ CPU @ 2.80GHz\n",
    "#  on Intel i7-7700HQ CPU @ 2.80GHz / Nvidia GeForce 940MX\n",
    "model = build_network(lambda x: AttentionWithContext()(\n",
    "                                keras.layers.Bidirectional(\n",
    "                                keras.layers.GRU(32, return_sequences=True))(x)))\n",
    "history_attn = model.fit(x_train, train_labels, batch_size=256, epochs=50, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 6min 32s on Intel i7-7700HQ CPU @ 2.80GHz\n",
    "# 13min 36s on Intel i7-7700HQ CPU @ 2.80GHz / Nvidia GeForce 940MX\n",
    "model = build_network(textcnn)\n",
    "history_tcnn = model.fit(x_train, train_labels, batch_size=256, epochs=50, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib standard palette\n",
    "# ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
    "\n",
    "import bqplot, bqplot.pyplot\n",
    "import ipywidgets\n",
    "\n",
    "axes_loss = {'x': {'label': 'Epochs'}, \n",
    "             'y': {'label': 'Losses', 'label_offset': '50px', 'tick_style': {'font-size': 10}}}\n",
    "axes_acc = {'x': {'label': 'Epochs'}, \n",
    "            'y': {'label': 'Accuracy', 'label_offset': '50px', 'tick_style': {'font-size': 10}}}\n",
    "\n",
    "loss_plt = bqplot.pyplot.figure(min_aspect_ratio=4/3, max_aspect_ratio=4/3)\n",
    "bqplot.pyplot.plot(range(30), history_embd.history['val_loss'], axes_options=axes_loss, colors=['#1f77b4'])\n",
    "bqplot.pyplot.plot(range(30), history_simplernn.history['val_loss'], colors=['#ff7f0e'])\n",
    "bqplot.pyplot.plot(range(30), history_gru.history['val_loss'], colors=['#2ca02c'])\n",
    "bqplot.pyplot.plot(range(30), history_attn.history['val_loss'], colors=['#d62728'])\n",
    "bqplot.pyplot.plot(range(30), history_tcnn.history['val_loss'], colors=['#9467bd'])\n",
    "bqplot.pyplot.plot(range(30), history_bigru.history['val_loss'], colors=['#8c564b'])\n",
    "bqplot.pyplot.plot(range(30), history_lstm.history['val_loss'], colors=['#e377c2'])\n",
    "\n",
    "acc_plt  = bqplot.pyplot.figure(min_aspect_ratio=4/3, max_aspect_ratio=4/3)\n",
    "bqplot.pyplot.plot(range(30), history_embd.history['val_acc'], axes_options=axes_acc, colors=['#1f77b4'])\n",
    "bqplot.pyplot.plot(range(30), history_simplernn.history['val_acc'], colors=['#ff7f0e'])\n",
    "bqplot.pyplot.plot(range(30), history_gru.history['val_acc'], colors=['#2ca02c'])\n",
    "bqplot.pyplot.plot(range(30), history_attn.history['val_acc'], colors=['#d62728'])\n",
    "bqplot.pyplot.plot(range(30), history_tcnn.history['val_acc'], colors=['#9467bd'])\n",
    "bqplot.pyplot.plot(range(30), history_bigru.history['val_acc'], colors=['#8c564b'])\n",
    "bqplot.pyplot.plot(range(30), history_lstm.history['val_acc'], colors=['#e377c2'])\n",
    "\n",
    "display(ipywidgets.HBox([loss_plt, acc_plt]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
