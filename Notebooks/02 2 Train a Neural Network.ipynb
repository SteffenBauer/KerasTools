{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "import numpy as np\n",
    "\n",
    "import bqplot.pyplot\n",
    "import ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data and preprocess it\n",
    "(train_images, train_labels), (test_images, test_labels) = keras.datasets.mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "train_labels = keras.utils.to_categorical(train_labels)\n",
    "test_labels = keras.utils.to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a neural network\n",
    "#\n",
    "# We will use a sequential model (one layer after another)\n",
    "# The network will accept as input flattened and normalized images of 1D size 784. \n",
    "# Two intermediate layers with 64 neurons and ReLU ('rectified linear unit') activation\n",
    "# The output layer will have 10 neurons and Softmax activation\n",
    "#\n",
    "# We will use 'categorical_crossentropy' for the loss function\n",
    "# The optimizer will be SGD ('stochastic gradient descent') with a learning rate of 0.01\n",
    "# We will additionally measure the predictive accuracy\n",
    "\n",
    "def build_network():\n",
    "    network = keras.models.Sequential()\n",
    "    network.add(keras.layers.Dense(128, activation='relu', input_shape=(28*28*1,)))\n",
    "    network.add(keras.layers.Dense(128, activation='relu'))\n",
    "    network.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    network.compile(optimizer=keras.optimizers.sgd(lr=0.01, momentum=0.9), \n",
    "                    loss='categorical_crossentropy', \n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a fresh new neural network and plot its architecture\n",
    "network = build_network()\n",
    "network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the network with the MNIST training set for 5 epochs and a batch size of 128\n",
    "# Depending on your hardware, training can take a while. Training progress is plotted for each epoch.\n",
    "# The fit() function returns the training loss/accuracy history of each epoch.\n",
    "\n",
    "epochs = 20\n",
    "history = network.fit(train_images, train_labels, epochs=epochs, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training, evaluate the generalizing power of the network with the test set. \n",
    "# How does it fare on data it hasn't seen before?\n",
    "\n",
    "test_loss, test_acc = network.evaluate(test_images, test_labels)\n",
    "print()\n",
    "print(\"Test loss: {}\".format(test_loss))\n",
    "print(\"Test accuracy: {}\".format(test_acc))\n",
    "history.history['test_loss'] = test_loss\n",
    "history.history['test_acc'] = test_acc\n",
    "history.history['epochs'] = epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss/accuracy training history and test results in some nice graphs\n",
    "\n",
    "x_epochs = range(1, len(history.history['loss']) + 1)\n",
    "y_loss = history.history['loss']\n",
    "y_acc = history.history['acc']\n",
    "axes_loss = {'x': {'label': 'Epochs'}, 'y':{'label': 'Loss'}}\n",
    "axes_acc = {'x': {'label': 'Epochs'}, 'y':{'label': 'Accuracy'}}\n",
    "\n",
    "fig1 = bqplot.pyplot.figure()\n",
    "bqplot.pyplot.plot(x_epochs, y_loss, axes_options=axes_loss)\n",
    "bqplot.pyplot.vline(epochs, line_style = 'dashed')\n",
    "bqplot.pyplot.scatter([epochs], [test_loss], colors=['red'], fill=False)\n",
    "bqplot.pyplot.xlim(0, epochs+1)\n",
    "\n",
    "fig2 = bqplot.pyplot.figure()\n",
    "bqplot.pyplot.plot(x_epochs, y_acc, axes_options=axes_acc)\n",
    "bqplot.pyplot.vline(epochs, line_style = 'dashed')\n",
    "bqplot.pyplot.scatter([epochs], [test_acc], colors=['red'], fill=False)\n",
    "bqplot.pyplot.xlim(0, epochs+1)\n",
    "\n",
    "display(ipywidgets.HBox([fig1, fig2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the network for later production use\n",
    "network.save(\"./mnist_trained.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
