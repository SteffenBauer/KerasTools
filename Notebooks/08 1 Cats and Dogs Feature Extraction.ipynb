{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import KerasTools as KT\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from keras import backend as K\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.mobilenet import preprocess_input\n",
    "\n",
    "# Use a MobileNet as convolutional base for feature extraction\n",
    "from keras.applications import MobileNet\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "# The example here uses the smallest possible mobilenet architecture, \n",
    "# but still achieves reasonable accuracy.\n",
    "#\n",
    "# Increase the alpha parameter and the image size \n",
    "# to get higher accuracy in the final network \n",
    "# for cost of longer extraction / training time\n",
    "alpha = 0.25                 # MobileNet model width, one of [0.25, 0.5, 0.75, 1.0]\n",
    "input_shape = (128, 128, 3)  # Image size, one of [128, 160, 192, 224]\n",
    "\n",
    "conv_base = MobileNet(weights='imagenet',      # Use pre-trained ImageNet weights\n",
    "                      include_top = False,     # No classifier, only the convolutional base: \n",
    "                      input_shape=input_shape, \n",
    "                      alpha=alpha,\n",
    "                      pooling='avg')\n",
    "\n",
    "# Needed here when called from a webapp like jupyter notebook; tensorflow does not play well with threads\n",
    "conv_base._make_predict_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(conv_base, directory, sample_count):\n",
    "    start_time = time.time()\n",
    "    batch_size = 10\n",
    "    features = np.zeros(shape=(sample_count, conv_base.output.shape[1]))\n",
    "    labels = np.zeros(shape=(sample_count, 2))\n",
    "    datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "    generator = datagen.flow_from_directory(\n",
    "              directory,\n",
    "              target_size=conv_base.input_shape[1:3],\n",
    "              batch_size=batch_size,\n",
    "              class_mode='categorical')\n",
    "    print(generator.class_indices)\n",
    "    i = 0\n",
    "    for inputs_batch, labels_batch in generator:\n",
    "        features_batch = conv_base.predict(inputs_batch)\n",
    "        features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
    "        labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
    "        i += 1\n",
    "        KT.update_progress(directory, float(i*batch_size)/sample_count)\n",
    "        if i * batch_size >= sample_count:\n",
    "          # Note that since generators yield data indefinitely in a loop,\n",
    "          # we must `break` after every image has been seen once.\n",
    "            break\n",
    "    end_time = time.time()\n",
    "    print(str(end_time-start_time)+ 's')\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '../Datasets/Pet_Dataset/train'\n",
    "test_dir  = '../Datasets/Pet_Dataset/test'\n",
    "\n",
    "train_features, train_labels = extract_features(conv_base, train_dir, 4800)\n",
    "test_features,  test_labels  = extract_features(conv_base, test_dir,  1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a custom DNN classifier\n",
    "def build_classifier():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(16, activation='relu', input_dim=int(alpha*1024)))\n",
    "    model.add(layers.Dense(2, activation='softmax'))\n",
    "    model.compile(optimizer=optimizers.sgd(momentum=0.9), loss='categorical_crossentropy', metrics=['acc'])\n",
    "    return model\n",
    "    \n",
    "classifier = build_classifier()\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the classifier using the extracted features\n",
    "batch_size = 200\n",
    "history = classifier.fit(train_features, train_labels, \n",
    "                         batch_size=batch_size, epochs=200,\n",
    "                         validation_split=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KT.plot_history(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the final classifier at the onset of overfitting\n",
    "final_epochs = 50\n",
    "\n",
    "classifier = build_classifier()\n",
    "classifier.fit(train_features, train_labels, \n",
    "               batch_size=batch_size, epochs=final_epochs)\n",
    "\n",
    "test_loss, test_acc = classifier.evaluate(test_features, test_labels)\n",
    "print()\n",
    "print('Test loss:', test_loss)\n",
    "print('Test accuracy:', test_acc)\n",
    "history.history['epochs'] = final_epochs\n",
    "history.history['test_loss'] = test_loss\n",
    "history.history['test_acc'] = test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KT.plot_history(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final model with both convolutional base & classifier\n",
    "model = models.Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(classifier)\n",
    "model.compile(optimizer=optimizers.sgd(momentum=0.9), loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "# Save model to disk\n",
    "model.save('./cats_and_dogs_trained.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
