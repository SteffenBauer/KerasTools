{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, threading, collections, io\n",
    "\n",
    "import ipywidgets\n",
    "import PIL.Image\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "import catch, snake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopEvent = threading.Event()\n",
    "currentGame = collections.deque([], 1)\n",
    "class GamePlot(threading.Thread):\n",
    "    def __init__(self, ratio=1.0):\n",
    "        threading.Thread.__init__(self, name=\"GamePlot\")\n",
    "        self.ratio=ratio\n",
    "        self.imbuf = io.BytesIO()\n",
    "        self.img = ipywidgets.Image(width=int(ratio*256), height=256)\n",
    "        display(self.img)\n",
    "        self.plot_frame(np.zeros((2,2), np.uint8))\n",
    "    def run(self):\n",
    "        while not stopEvent.isSet():\n",
    "            try: game = currentGame.pop()\n",
    "            except IndexError: time.sleep(0.1)\n",
    "            else:\n",
    "                for frame in game:\n",
    "                    self.plot_frame(frame)\n",
    "                    time.sleep(0.1)\n",
    "                time.sleep(0.5)\n",
    "    def plot_frame(self, frame):\n",
    "        self.imbuf.seek(0)\n",
    "        frame *= 255\n",
    "        PIL.Image.fromarray(frame.astype('uint8')).\\\n",
    "                  resize((int(self.ratio*256), 256)).\\\n",
    "                  save(self.imbuf, 'png')\n",
    "        self.img.value = self.imbuf.getvalue()\n",
    "        \n",
    "class GameStore:\n",
    "    def __init__(self, ratio=1.0):\n",
    "        currentGame.append([])\n",
    "        self.plotter = GamePlot(ratio)\n",
    "        self.plotter.start()\n",
    "    def game_start(self, frame):\n",
    "        self.gamestore = [frame]\n",
    "    def game_frame(self, frame):\n",
    "        self.gamestore.append(frame)\n",
    "    def game_over(self):\n",
    "        currentGame.append(self.gamestore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size, width, height, nb_frames = 12, 12, 12, 1\n",
    "game = catch.Catch(grid_size, movement_cost=-0.01)\n",
    "\n",
    "#grid_size, width, height, nb_frames = 10, 10, 10, 1\n",
    "#game = snake.Snake(grid_size)\n",
    "\n",
    "learning_rate = 0.001\n",
    "def loss_fn(y_true, y_pred):\n",
    "    return -1.0 * keras.backend.sum(y_true * keras.backend.log(y_pred+1e-20))\n",
    "\n",
    "def model_dnn():\n",
    "    inp = keras.layers.Input(shape=(nb_frames, height, width, 3), name='state_input')\n",
    "    flt = keras.layers.Flatten(name='flat')(inp)\n",
    "    x = keras.layers.Dense(16, activation='relu', name='dense1')(flt)\n",
    "    x = keras.layers.Dense(16, activation='relu', name='dense2')(x)\n",
    "    act = keras.layers.Dense(game.nb_actions, activation='softmax', name='actions')(x)\n",
    "\n",
    "    model = keras.models.Model(inputs=inp, outputs=act, name='PG_DNN')\n",
    "    model.compile(loss=loss_fn, optimizer=keras.optimizers.RMSprop(learning_rate))\n",
    "    \n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def model_rcnn():\n",
    "    cs = 32\n",
    "    inpc = keras.layers.Input(shape=(height, width, 3), name='conv_input')\n",
    "    conv1 = keras.layers.Conv2D(cs,3,padding='same',strides=2,activation='relu', name='conv1')(inpc)\n",
    "    conv2 = keras.layers.Conv2D(cs*2,3,padding='same',strides=2,activation='relu', name='conv2')(conv1)\n",
    "    flat = keras.layers.Flatten(name='flatten')(conv2)\n",
    "    convm = keras.models.Model(inputs=inpc, outputs=flat, name='CONV_BASE')\n",
    "    convm.summary()\n",
    "\n",
    "    ls = cs\n",
    "    inp = keras.layers.Input(shape=(nb_frames, height, width, 3), name='state_input')\n",
    "    x = keras.layers.TimeDistributed(convm, name='conv_distributed')(inp)\n",
    "    x = keras.layers.SimpleRNN(ls, return_sequences=False, name='rnn')(x)\n",
    "    act = keras.layers.Dense(game.nb_actions, activation='softmax', name='actions')(x)\n",
    "\n",
    "    model = keras.models.Model(inputs=inp, outputs=act, name='PG_RCNN')\n",
    "    model.compile(loss=loss_fn, optimizer=keras.optimizers.RMSprop(learning_rate))\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def train(model, game, episodes=512, log_freq = 100, gamma=0.98, callbacks=[]):\n",
    "    time_steps = []\n",
    "    win_stats = []\n",
    "    loss_stats = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        game.reset()\n",
    "        start_frame = game.get_frame()\n",
    "        all(c.game_start(start_frame) for c in callbacks)\n",
    "        curr_state = game.get_state()\n",
    "        done = False\n",
    "        transitions = [] # list of state, action, rewards\n",
    "    \n",
    "        for t in range(game.max_turn): #while in episode\n",
    "            act_prob = model.predict(np.expand_dims(np.asarray([curr_state]), axis=0))\n",
    "            action = np.random.choice(np.array([0,1,2]), p=act_prob[0])\n",
    "            prev_state = curr_state\n",
    "            curr_state, reward, done = game.play(action)\n",
    "            curr_frame = game.get_frame()\n",
    "            all(c.game_frame(curr_frame) for c in callbacks)\n",
    "            transitions.append((prev_state, action, reward))\n",
    "            if done:\n",
    "                all(c.game_over() for c in callbacks)\n",
    "                win_stats.append(1 if game.is_won() else 0)\n",
    "                break\n",
    "\n",
    "        # Optimize policy network with full episode\n",
    "        ep_len = len(transitions) # episode length\n",
    "        discounted_rewards = np.zeros((ep_len, game.nb_actions))\n",
    "        train_states = []\n",
    "        for i in range(ep_len): #for each step in episode\n",
    "            discount = 1.0\n",
    "            future_reward = 0.0\n",
    "            # discount rewards\n",
    "            for i2 in range(i, ep_len):\n",
    "                future_reward += transitions[i2][2] * discount\n",
    "                discount = discount * gamma\n",
    "            discounted_rewards[i][transitions[i][1]] = future_reward\n",
    "            train_states.append([transitions[i][0]])\n",
    "        train_states = np.asarray(train_states)\n",
    "        # Backpropagate model with preds & discounted_rewards here\n",
    "        loss = model.train_on_batch(train_states, discounted_rewards)\n",
    "        loss_stats.append(loss)\n",
    "    \n",
    "        if len(win_stats) >= log_freq:\n",
    "            print(\"Episode {: 6d} Win perc {: 4.2%} Loss {: 2.8f}\".format(\n",
    "                episode+1, sum(win_stats)/float(log_freq), sum(loss_stats)/float(log_freq))\n",
    "            )\n",
    "            win_stats = []\n",
    "            loss_stats = []\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = model_rcnn()\n",
    "game.max_turn=16\n",
    "stopEvent.clear()\n",
    "train(model, game, episodes=10000, log_freq=1000, gamma=0.95, callbacks = [GameStore(ratio=width/height)])\n",
    "stopEvent.set()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
